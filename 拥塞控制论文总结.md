# 拥塞控制论文

## A Machine Learning-based Framework for Dynamic Selection of Congestion Control Algorithms（基于机器学习的拥塞控制算法动态选择框架）

**Published in:** [IEEE/ACM Transactions on Networking](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=90) ( Volume: 31, [Issue: 4](https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=10223306&punumber=90), August 2023) 

### abstract

我们提出了一个系统，在特定环境中针对特定流的最合适的cca之间动态切换。Antelope是一个可以动态重新配置堆栈以为单个流使用最合适的CCA的系统。我们建立了一个机器学习模型来学习哪种算法最适合各个条件，并实现内核级支持在cca之间动态切换。该框架还考虑了应用程序的性能需求，以便根据应用层需求对选择进行微调。此外，为了减少机器学习在单个前端服务器上引入的开销，我们(可选地)在云中实现了CCA选择过程，这允许在前端服务器之间共享模型和选择。结果表明，Antelope通过动态调整单个流量的cca来证明其有效性。

核心：机器学习，内核实现切换cca，可以动态重新配置堆栈，还考虑了应用程序的性能需求

### introduction

Antelope根据观察到的网络和流状态调整单个流的拥塞控制算法。它从内核数据路径收集TCP流信息，并将信息传递到用户空间，在那里我们可以利用已有的机器学习库。使用监督分类，Antelope然后预测哪种CCA可以达到特定流的最佳性能，该预测还考虑了应用层的需求(例如:延迟敏感或吞吐量敏感)考虑在内。然后继续监控流量，并在网络环境或流量状态发生变化时动态地更改CCA。

但这个扩展版本为Antelope增加了两个新的增强功能:(i)结合应用程序的性能偏好和基于云的远程学习;(二)几组新的实验。

贡献：

我们设计并实现了Antelope，这是一个自适应CC框架，它可以在每个流的基础上动态地在最合适的CCA之间进行重新配置。Antelope只需要更改TCP发送方而不更改TCP套接字。因此，Antelope可以很容易地部署到生产环境中，并且其源代码可供社区使用。

作为Antelope的一部分，我们构建并训练了一个监督分类算法(在用户空间中)，该算法可以为与训练数据具有相似模式的流选择合适的cca，也可以在以前没有出现过的流上选择合适的cca。我们表明，eBPF作为Antelope的一部分，是在内核中管理cca的有效选择，即使在建立了TCP流之后也是如此。选择也可以在集中式云服务器中实现。通过这样做，模型和选择可以在服务于单个流的前端服务器之间共享，因此分摊了机器学习训练和处理引入的额外开销。

在广域网(WAN)、数据中心网络(DCN)和蜂窝网络中进行的大量实验表明，与BBR相比，Antelope的吞吐量平均提高了16%;与CUBIC相比，Antelope平均提高了19%的吞吐量，减少了10%的延迟。此外，Antelope表现出比最先进的基于ml的机制(Orca和PCC-Vivace)更好的性能。实验还证明了基于云的共享远程学习在减少前端服务器开销方面的好处。

### motivation

应用程序可能有不同的性能需求。例如，在线聊天和云游戏对延迟更敏感，而传统的视频点播服务对吞吐量更敏感。

### Antelope overview

工作原理：antelope将在这三个阶段执行不同的动作。在连接建立之后，Information Collection组件(在内核中)将收集TCP流信息并将其传递给Mechanism Match组件(在用户空间中)。然后在数据传输阶段，机制匹配组件(周期性地)根据流的特征选择最合适的CCA。然后，最合适的CCA将被传递给Mechanism Switch组件(在内核中)，该组件将切换到网络堆栈中的该CCA。当连接关闭时，机制匹配和机制开关组件都将删除此流的记录。

信息收集。信息收集组件包括两个子模块:数据收集模块和数据处理模块。Data Collection模块在内核中运行。它收集所有的TCP流信息，然后通过eBPF传递给位于用户空间的Data Process模块。Data Process模块在将数据传递给Mechanism Match组件之前聚合并格式化数据。在第五节中，我们将说明我们如何收集信息。

匹配机制。机制匹配组件包括两个子模块:在线预测和离线训练模块，这两个模块都在用户空间中实现。当TCP信息传递给Mechanism Match组件时，它将动态地选择要使用的最合适的CCA。然后，这将被记录到bpf映射结构中，并在内核中进行访问。在线预测模块依赖于几个训练模型来选择不同的机制，并将根据每个模型生成的分数返回最合适的一个。为了通知这个过程，离线训练模块将使用奖励函数训练匹配的模型。具体来说，我们使用XGBoost构建决策树模型。match组件还考虑不同应用程序的性能首选项。一些应用程序(例如在线聊天)对延迟敏感，而另一些应用程序(例如文件传输)对吞吐量敏感。**Match组件向应用程序公开API，以获取应用程序的性能首选项。**然后，我们在在线预测模块中通过考虑流和网络状态以及应用需求来预测合适的cca。具体来说，**奖励函数通过对吞吐量和延迟分配不同的权重来反映应用程序的需求。该组件的详细信息见第四节。**请注意，该组件也可以在云中实现，其中多个前端服务器可以共享一个组件，以减少单个服务器上的计算开销。

切换机制。Mechanism Match组件记录流标识符(按IP和端口)以及所考虑的流的相应CCA。使用eBPF，这些信息被传递给内核。然后，Mechanism Switch组件(在内核中)将切换到所选的CCA。这个进程连接到三个Linux内核函数:tcp setup、tcp sendmsg和tcp close。在tcp setup和tcp sendmsg函数中，钩子监视bpf映射，并根据指示切换cca。在tcp close中，钩子函数向机制匹配和机制切换组件发送流关闭信号。

### prediction and training

antelope接受一组连续的N个ACK数据包作为输入(按照ACK数据包到达的顺序)。我们把这组数据包称为一个数据单元。然后对每个数据单元的粒度执行CCA选择。一旦记录到N个包，信息就会传递给选择模块和奖励模块。选择模块由多个不同cca的预测模型组成。通过比较每个模型的回报预测，选择最佳的CCA。当产生下一个数据单元时，奖励模块分析其统计数据以评估上一次切换CCA的效果。

（跳过机器学习的部分）

统计模块。统计模块负责收集流量信息。它通过从information Collection组件读取流的信息来实现这一点。在接收到ACK后，内核更新流的信息，例如RT T, CW ND，发送速率，丢失的包数。设dt表示流中的第t个数据单元，st表示dt的统计量。对于每个数据单元(默认为每20个数据包，即N = 20)，我们根据每个ACK数据包收集到的流信息计算统计信息。我们将数据单元大小设置为计算开销和效率之间的折衷。注意，在每个数据单元的基础上计算统计信息(而不是每个ACK的统计信息)可以减少基于机器学习的决策中网络噪声的影响[32]。表一列出了统计数字的摘要

**CCA的有效性可以通过一个称为Power的指标来衡量，该指标定义为Power =吞吐量/延迟。**结果表明，当功率达到最大值时，不仅网络处于最佳状态，单个流也处于最佳状态。我们还将损失作为一个参数来调整奖励函数，以使丢包最小化。在计算奖励函数时，我们将吞吐量单位设置为Kbps，延迟设置为ms，损失设置为丢失的数据包数量(在一个数据块间隔内)。η是决定丢包对奖励函数权重的参数。在我们当前的实现中，我们经验地将其设置为1。

TCP连接建立完成后，δ将初始化为2。当信息包被信息收集组件接收时，δ将随着数据单元的数量呈指数增长。例如，第一个数据单元的δ为2，第二个数据单元后的δ为4，等等。这意味着当流中发送更多的数据包时，奖励函数将从延迟敏感变为吞吐量敏感

最后，吞吐量定义如Eq. 3所示，其中吞吐量是测量的平均吞吐量，MAX RAT E是起跳速率MAX的最大值(在内核中定义为2 23−1)，参数ζ和θ用于捕获吞吐量和延迟的应用程序偏好(ζ∈{0,1}和θ∈{0,1})。具体来说，**我们定义了应用程序性能首选项的三种模式:延迟敏感模式、吞吐量敏感模式和默认模式。**

在默认模式下，ζ和θ都设置为1，这意味着具有良好吞吐量和低延迟的CCA应该具有更高的奖励值(因此应该选择)。对于延迟敏感模式，ζ和θ分别设为0和1;然后选择具有最低延迟的cca。另一方面，对于吞吐量敏感模式，ζ和θ分别设为1和0;羚羊将选择具有最佳吞吐量的cca。通过这两个参数，Antelope在选择最合适的cca时考虑应用程序的首选项(吞吐量或延迟敏感)。

选择模块。这个模块负责从一组可用的cca(对于给定的流)中检索奖励预测，然后选择最优的一个。然而，短的TCP流可能在奖励模块计算预测之前就完成了。因此，**我们使用两种类型的预测:(1)流级预测，通过分析实时信息来预测该流最合适的CCA(适用于长流);(2) IP级预测，它使用来自该IP地址或前缀的有关先前流的历史信息(适用于短流)。我们在下面描述这些。**

机器学习预测：XGBoost

算法2给出了ip级预测伪代码。对于每个IP范围，Antelope记录在流向该IP空间的流中选择每个CCA的次数。为了适应网络的变化，每次获得新的流级预测时，将IP预测结果与当前预测结果合并。注意，历史数据的加权系数为α (0 < α < 1)，该系数与数据的年龄成反比(数据越老，权重越低)。最后，选择最频繁且奖励最高的CCA。

### implementation

主要讲了用户内核映射

在cca之间动态切换时，确保流量参数的连续性是很重要的。例如，新的共同承诺协议应与前一共同承诺协议的CWND一起启动。两种参数与这种连续性有关:(i)常见参数，如发送速率(CWND或起搏速率);(ii) RTT、丢包率等测量参数。在Linux内核中，上述参数记录在结构体sock中，由所有cca维护。对于发送速率，当切换到新的CCA时，我们使用与之前相同的值。对于测量参数，我们也继承相同的值。这是因为不同的cca使用相同的模块来计算这些参数，因此不会影响测量参数的精度。为了解决这个问题，由于所有这些参数都有默认值，

应用程序需求：Antelope使用API来接收应用程序需求，即图2中的应用程序需求API。API有两个变量:键和类型。我们将应用程序的端口号设置为键，以区分它是哪个应用程序。Type设置为1、2或3，分别表示默认(同时考虑吞吐量和延迟)、延迟或吞吐量敏感模式。该API可以在开发人员创建TCP套接字后使用。此信息传递给Antelope的Mechanism Match组件，然后用于在Eq. 1中为不同的流设置吞吐量和延迟的权重。通过这种方式，羚羊在选择相应的cca时将显示对延迟或吞吐量的偏好。请注意，对于每个CCA，我们训练了3个模型，对应于3个应用程序偏好模式。类型参数指示应该使用哪个模型进行CCA选择。如果应用程序没有设置API，默认情况下，Antelope将同时考虑延迟和吞吐量，并选择在延迟和吞吐量之间取得平衡的CCA。

### training and experimentation

实验，没啥思路部分，跳过

我们首先展示Antelope如何在cca(包括BBR, CUBIC, C2TCP, Vegas, Illinois和Westwood)之间切换，以及TCP参数如何变化。然后，我们描述了羚羊在评估和生产环境中的性能。

最后，我们还比较了另一种CC切换机制Rein[29]，该机制使用基于规则的算法来选择CCA。由于Rein的源代码不开放，我们按照论文中提供的算法实现Rein:默认使用CUBIC，在小型缓冲网络中切换到BBR, WiFi连接切换到Westwood。

G.验证参数选择

回想一下，在选择cca时，η定义了丢包的影响，δ决定了延迟对奖励函数的影响。接下来，我们检查η(默认1)和δ(默认2)的最佳设置。图16展示了模拟DCN、WAN和蜂窝网络的性能(吞吐量、延迟)。对于不同的环境，我们用不同的η和δ值进行实验。从图中可以看出，当η值为1时，流体的性能优于η值为0.5和1.5时。当η值为0.5时，蜂窝网络的性能最差。这是因为蜂窝网络具有较高的丢包率，0.5的值使得奖励函数对丢包率不那么敏感。当δ设置为1时，羚羊实现了较小的延迟，但也降低了吞吐量。另一方面，当δ为3时，流的吞吐量变化很大。例如，在蜂窝环境中，吞吐量较大，而对于DCN和WAN，吞吐量较小。这是因为在这种情况下，第二个数据单元的δ将为6，这将使流的性能更加可变。因此，我们设置δ的初始值为2，以减少波动。

### related work

TCP品种。我们并不是第一个发现cca可以针对不同的环境进行优化的人。例如，Sprout[7]、C2TCP[8]、ExLL[39]和PBE-CC[40]都是专门为蜂窝网络设计的。同样，DCTCP[11]、pFabric[12]、Timely[41]和Swift[13]也是通过使用显式拥塞通知[42]为数据中心网络设计的。BFC[43]通过逐跳控制在数据中心实现接近最优的吞吐量和尾部延迟行为。TCPLS通过在TCP中提供多路复用、连接迁移服务，为应用程序提供了更多的控制[44]。Orca[20]、Libra[45]、TCP-Drinc[46]和[23]使用深度强化学习调整cca的参数(如拥塞窗口大小)。在我们的工作中，我们没有尝试设计新的cca或调整其参数，而是根据需要动态选择观察到的网络条件的最佳算法。DCTCP[11]、pFabric[12]和ACC[47]使用ECN来推断网络状态(如缓冲区和通道容量)，然后调整拥塞。这种机制需要路由器的支持。我们的机制只需要终端主机的支持，因此更容易部署。

最佳cca的选择。与Antelope最相关的是Rein[29]，它也试图为不同的网络选择最合适的cca。雷小山依赖于基于规则的选择。它首先对网络环境(例如WiFi或有线)进行分类，并使用手动指定给该环境的CCA。相比之下，Antelope通过机器学习更准确地预测cca。此外，Rein使用管道在用户空间和内核之间交换信息，而Antelope依赖于eBPF。这使得使用新的cca和学习机制扩展Antelope变得更加容易。TCP-RL[48]是另一项使用强化学习选择合适cca的工作。然而，TCP-RL完全在基于Pantheon的用户空间中实现选择[30]，这意味着需要单个应用程序实现支持。为了提高CDN性能，Configanator[49]为web服务器的配置参数(如cca、initRTO、时间戳、HTTP版本和HTTP最大帧大小)提供了编程控制。Antelope只针对cca，而Configanator针对整个web服务器。Antelope可以通过其内核级集成扩展到任何应用程序(不仅仅是web服务器)。

### conclusion

Antelope使用机器学习与网络环境，流状态以及应用程序需求作为输入来预测最合适的cca。

## Antelope: A Framework for Dynamic Selection of Congestion Control Algorithms（羚羊:一个动态选择拥塞控制算法的框架）

**Published in:** [2021 IEEE 29th International Conference on Network Protocols (ICNP)](https://ieeexplore.ieee.org/xpl/conhome/9651731/proceeding)

### abstract

我们提出了一个系统，以动态切换最适合的拥塞控制机制之间的特定环境中的特定流。这带来了许多挑战，我们通过Antelope的设计和实现来解决这些挑战，Antelope是一个可以动态重新配置的系统，可以为单个流使用最合适的拥塞控制机制。我们构建了一种机器学习方法来学习哪种算法最适合各个条件，并实现了对动态调整拥塞控制算法的内核级支持。

### introduction

我们的目标是设计一个拥塞控制框架，它可以在所有环境和需求中实现良好的性能，并具有足够的灵活性，可以随着时间的推移而发展。Antelope根据观察到的网络和流状态调整单个流的拥塞控制算法。

我们构建并训练了一个监督分类算法(在用户空间中)，该算法可以为与训练数据具有相似模式的流选择合适的CC机制，也可以为以前未出现的新流选择合适的CC机制。我们表明，eBPF作为Antelope的一部分，是在内核中管理CC算法的有效选择，即使在TCP流已经建立之后也是如此。

### motivation

### Antelope overview

信息收集。信息收集组件包括两个子模块:数据收集模块和数据处理模块。Data Collection模块在内核中运行。它收集所有的TCP流信息，然后通过eBPF传递给位于用户空间的Data Process模块。Data Process模块在将数据传递给Mechanism Match组件之前聚合并格式化数据。在第五节中，我们将说明我们如何收集信息

匹配机制。机制匹配组件包括两个子模块:在线预测和离线训练模块，这两个模块都在用户空间中实现。当TCP信息传递给Mechanism Match组件时，它将动态地选择要使用的最合适的CC算法。然后，这将被记录到bpf映射结构中，并在内核中进行访问。在线预测模块依赖于几个训练模型来选择不同的机制，并将根据每个模型生成的分数返回最合适的一个。为了通知这个过程，离线训练模块将使用奖励函数训练匹配的模型。具体来说，**我们使用XGBoost构建了一个决策树模型。**这一组成部分的详细情况见第四节。

切换机制。当机制匹配组件选择最合适的机制时，**它将记录流标识符(按IP和端口)和相应的CC机制。使用eBPF，信息被传递到内核**。然后，Mechanism Switch组件(在内核中)将切换到所选的CC算法。这个进程连接到三个Linux内核函数:tcp setup、tcp sendmsg和tcp close。在tcp setup和tcp sendmsg函数中，钩子监视bpf映射，并根据指示切换CC机制。在tcp close中，钩子函数只向机制匹配和机制切换组件发送流关闭信号。

### prediction and training

预测模块：统计模块，奖励模块，选择模块

此外，不同大小的流对吞吐量和延迟的敏感性差异很大。例如，大的流通常是吞吐量敏感的，但小的流更关心延迟。为了解决这个问题，我们将系数δ(≥1)添加到Eq 2中(它定义了Eq. 1中使用的delay0):TCP连接建立完成后，δ将初始化为2。当信息包被信息收集组件接收时，δ将随着数据单元的数量呈指数增长。例如，第一个数据单元的δ为2，第二个数据单元后的δ为4，等等。这意味着当流中发送更多的数据包时，奖励函数将从延迟敏感变为吞吐量敏感。Orca[5]使用了类似的方法。

选择模块。这个模块负责在一组可用的CC机制(对于给定流)中检索奖励预测，然后选择最优的奖励预测。然而，短的TCP流可能在奖励模块计算预测之前就完成了。因此，我们使用两种类型的预测:(1)流级预测，通过分析实时信息来预测最适合该流的CC机制(适用于长流);(2) IP级预测，它使用来自该IP地址的有关先前流的历史信息(适用于短流)。我们在下面描述这些。

训练模块：离线训练，在线训练

### implementation

A收集流量信息：我们使用BPF Compiler Collection (BCC)探测函数来获取TCP流信息[1]。我们从内核中的结构体sock中提取信息。BCC在Linux网络堆栈中设置了不同的钩子函数，这意味着我们可以从不同的钩子点获取信息。在我们的系统中，我们在tcp返回函数中设置了一个钩子。

B通过ebpf映射交换信息

C在内核中切换TCP：在线预测模块中，一旦收到N个ACK报文，就会触发预测过程(默认N = 20)。如果预测过程找到另一种适合此流的机制，它将更新ebpf映射，并添加IP+流ID ->地图中的拥塞机制项。Antelope可以在主流Linux内核中实现的CC机制之间切换，目前包括BBR, Cubic, C2TCP, Vegas, Illinois和Westwood。不管竞争的TCP流是否使用Antelope，单个流可能使用不同的cc。因此，Antelope继承了所选CC算法的公平性。

### training and experimentation

A测试平台：虚拟，真实

B训练：虚拟，真实

C表现评估：验证切换机制,仿真网络中的性能评估，真实网络性能评估

D开销：Antelope的开销包括三个部分:(1)用户空间的学习开销;(2)通过eBPF进行信息交换;(3)内核CC机制切换。

### related work

TCP种类，最佳CC机制的选择，TCP在内核中的实现。

### conclusion

**Antelope是一个基于每个流学习合适的CC算法的系统。**我们已经证明Antelope可以成功地在各种网络类型中应用最优或接近最优的CC算法。通过这种方式，我们可以在不需要管理员手动配置堆栈的情况下提高性能。Antelope为CC算法的动态选择铺平了道路。我们还注意到，某些应用程序集成了自己的控制回路来处理拥塞(例如视频流)。因此，**我们很想了解Antelope如何与这些应用程序进行互操作。**

## A Unified Congestion Control Framework for Diverse Application Preferences and Network Conditions（针对不同应用偏好和网络条件的统一拥塞控制框架）

[CoNEXT '21: Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies](https://dl.acm.org/doi/proceedings/10.1145/3485983)December 2021Pages 282–296https://doi.org/10.1145/3485983.3494840

### abstract

在本文中，我们提出了Libra，这是一个统一的拥塞控制框架，**它通过结合经典和基于强化学习(RL)的cca的智慧，赋予了灵活性、适应性和实用性。**同时，Libra可以灵活地满足不同的应用需求，

### introduction

一些应用程序，如云存储复制和软件下载，是面向吞吐量的，而其他应用程序，如VR/AR和云游戏，则是延迟敏感的。另一方面，网络通信是通过不同类型的网络(例如，蜂窝网络、WIFI或光纤)和不同的区域(例如，内部或洲际)进行的。因此，现代CCA应该适应这种异质性，与时俱进。

一些应用程序，如云存储复制和软件下载，是面向吞吐量的，而其他应用程序，如VR/AR和云游戏，则是延迟敏感的。即专门为有线网络设计的面向吞吐量的cca(例如CUBIC)通常在蜂窝网络中表现不佳[1,3,34,40]。

**同时，效用函数是一个潜在的接口，可以相应地调整以反映不同的应用程序需求。**

 **(2) Flexibility: adjust performance preferences according to the application preferences.**

为了同时满足这些目标，我们提出了Libra，这是一个统一的拥塞控制框架，结合了经典和基于rl的cca的智慧。Libra的动机如下:(1)经典cca和基于rl的cca具有互补的优势。基于rl的cca在适应性和灵活性方面具有巨大的潜力，无需手动设计。经典的cca可以帮助他们实现这些愿景，同时在实践中减轻他们的问题。Libra通过利用一个新的三阶段框架来解决这个问题，该框架有效地评估和利用底层cca的决策。在Libra中，底层的经典CCA有助于实现协议间公平性和安全保障的实用性。基于rl的CCA提高了自适应性能。我们设计的组合框架具有低开销、高性能和可证明的协议间公平性和收敛性。具体而言，我们的贡献如下。

首先，我们通过实验展示了现有cca的局限性，并通过一些见解详细描述了Libra的设计。Libra采用三阶段控制周期，其中(1)探索网络条件，(2)使用我们基于实用程序的框架评估经典和基于学习的cca的性能，以及(3)利用先前的决策并确定下一个控制周期的基本发送速率。虽然基于效用的框架在高层次上类似于PCC[12]，**但我们的新颖之处在于重新设计了包含速率控制组件的算法，并对使用我们的组合机制进行了全面的探索。**通过这些改进，Libra具有比PCC更好的适应性，更快的收敛速度和更低的开销。

其次，考虑到RL公式的影响(即奖励函数、动作空间和状态空间的设计)的文献有限，我们通过实验总结了RL公式的关键观察结果，**并相应地优化了基于RL的组件**，以提高Libra的性能。然后，我们讨论了经典cca顺利集成的细节，并提供了调优参数的指导方针。

最后，我们在Linux内核中实现了Libra，并在模拟网络和实时互联网中使用最先进的cca对其进行了评估。实验结果表明，Libra可以(1)在各种网络中始终优于最先进的(例如，吞吐量是Orca的1.2倍)，(2)对缓冲区大小，随机损失和参数设置的敏感性较低，(3)根据应用需求灵活调整性能偏好，(4)与其他基于学习的cca相比，CPU利用率降低约92%。(5)在协议间和协议内公平性方面，Jain的公平性指数保持在98%以上，并迅速收敛到公平份额。

### preliminaries and motivation

在本节中，我们通过实验展示了现有cca的局限性。实验使用Pantheon[38]，一个社区评估平台，具有候选CCAs的默认参数。在这里，我们使用CUBIC作为Libra的基础经典CCA。

另一方面，对于传统的cca，调整应用程序首选项的接口是关闭的。**尽管基于学习的方法在一定程度上实现了这种接口，但它们只关注特定的需求(吞吐量或延迟[3,25])，不能同时调整多个偏好。**

经典的cca具有较好的实用性，但难以实现自适应性和灵活性。基于学习的CCAs本质上可以实现良好的适应性，但也面临着一系列的实际问题。经典cca和基于学习的cca的互补优势促使我们提出一个组合框架。请注意，我们使用强化学习是因为它是顺序的和有远见的，它很适合CC问题的顺序决策过程[1,24]。

### libra overview

在高层次上，Libra采用基于效用的框架来量化不同发送速率的性能，计算出每个候选的效用值，最终选择最佳的一个。通常，**这个效用函数包括三个关键变量——吞吐量、延迟和损失[22]**——来捕捉网络条件，其目标是奖励导致更低延迟、更少损失和更高吞吐量的行为[12]。

基本上，在探索阶段，Libra最初将基础发送速率设置为上一个周期的赢家，并遵循经典的CCA来调整发送速率。同时，将网络反馈信息输入到基于学习的CCA中作为备份。在评估阶段，Libra会在一段时间内逐一尝试经典CCA和基于学习的CCA的发送率进行评估。最后，在开发阶段，Libra使用上一个控制周期确定的初始基本发送速率，等待评估阶段候选速率的反馈信息。通过收集统计数据并将其转化为效用值，Libra选择最佳发送速率，并将其作为下一个控制周期的新基准发送速率。由于网络条件(即可用带宽或竞争流的数量)可能在中途发生变化，或者偶尔会延迟ack，因此Libra发送方在决策间隔内可能不会收到任何ack[2]。天秤座处理这些问题的方式不同。具体来说，如果在探索阶段没有收到ACK, Libra将跳过相应的动作，并对基于学习的CCA保持相同的速率决策(x𝑟𝑙)。而在其他阶段没有收到ACK的情况下，Libra无法准确地评估速率决策，因此在下一个控制周期中重复使用当前的基本发送利率x𝑝𝑟𝑒𝑣。

### libra design

4.1组合框架：探索阶段:在这个阶段，从上一个控制周期的基本发送速率x𝑝𝑟𝑒𝑣开始，一个经典的CCA以每ack的方式更新Libra的发送速率。同时，将网络反馈信息输入到基于学习的CCA中，并以每个mi的方式作为备份。我们的目的是从经典cca和基于学习的cca中探索网络条件，并互补各自的优势。评价阶段:libra用两个评价间隔(ei)来解决评价阶段的分歧。如图3中间所示，一个EI应用了经典CCA的率值x𝑐𝑙，另一个EI应用了基于学习的CCA的率值x𝑟𝑙。同时，Libra对网络统计数据进行汇总，计算出一个与探索阶段的行为相对应的效用值𝑢(x𝑝𝑟𝑒𝑣)，用于与开发阶段结束时的𝑢(x𝑐𝑙)和𝑢(x𝑟𝑙)进行比较。与第一阶段不同，Libra不再同时计算经典CCA和基于学习的CCA的发送率。这可以通过减少昂贵的基于学习的计算而大大降低计算开销，从而有利于实用性(第5.3节)。

在0 <𝑡< 1偏好参数𝛼𝛽,𝛾> 0,𝑥𝑖是发送方i的发送速率𝐿是观察到的损失率。我们认为，通过合理的设计，Libra可以收敛到一个独特的平衡点[10]——每个发送者保持一个稳定和公平的发送率，具有最大的效用价值。

4.2基于RL的CCA：**我们的目标是为基于rl的CCA设计一个更好的状态空间**。在此基础上，我们评估了之前基于学习的cca中使用的不同状态空间的性能，如图5所示。我们可以观察到，在所有cca中，DRL-CC和PCC的状态空间获得了更高的奖励。因此，我们进一步探索PCC和DRL-CC中使用的状态空间，并通过统一它们的状态来产生基线状态(iv)， (vi)， (vii)， (viii)和(ix)。基于rl的cca的动作集遵循AIAD模式(例如，RL-TCP和DRL-CC)或MIMD模式(例如，Aurora和Orca1)。具体来说，AIAD模式下的动作集在开始学习策略时会消耗更多的集数。相反，具有MIMD模式的动作集学习策略更快，可以快速收敛到稳定状态。尽管如图6(b)所示的奖励有轻微波动，但我们可以通过我们的组合框架来缓解它，并最终在基于rl的CCA中选择MIMD动作空间。基于rl的CCA本身很难显著提高公平性，这激发了我们整合经典的CCA，并使用组合的方法来提高公平性。同时，我们更新了最大吞吐量变量(𝑚𝑎)和最小延迟变量(𝑑𝑚)(𝑛)来规范化奖励(第6行)。

**此外，考虑到发送方只能从观察到的特征[2]中推断出网络的统计信息，我们构建了包含多个先前状态的状态向量，而不是只使用最近的状态向量，从而允许发送方捕获序列数据中的依赖关系并检测网络状态[20]的变化。因此，我们结合了归一化特征向量𝑓并将状态向量表述为𝑆=⟨𝑓𝑡− +1，𝑓𝑡− +2，···，𝑓𝑡⟩。**

4.3传统CCA：作为一个组合框架，Libra将经典CCA作为子程序(图1)。挑战在于如何在不损害经典CCA自身优势的情况下整合经典CCA，这涉及到在整合不同CCA时统一基于窗口或基于费率的方案，同时设置退出Libra探索阶段的持续时间和阈值。首先，大多数经典cca(例如CUBIC)基于当前发送速率计算增加或减少，而不是从头开始计算。这些cca可以很容易地集成到Libra中，几乎不需要修改。我们为他们设置了一个RTT的探索阶段，这使Libra能够探索广泛的可用带宽，同时快速响应网络动态。一个特殊的例子是BBR，它通过将发送速率调整为1.25×、0.75×和1× 6次来探测可用带宽，总共8个rtt。我们将BBR控制回路的前三个rtt继承到Libra的探测阶段，因为它们体现了带宽探测过程的主要功能。其次，阈值(𝑡ℎ1)被设置为0.3×基本发送速率，以覆盖BBR的带宽探测阶段(±0.25×速率)，并对底层经典cca观察到的严重拥塞做出反应。

### evaluation

强化学习部分跳过

5.1适应性表现：不同痕迹的影响，不同缓冲区大小的影响，随机丢包的影响

**5.2灵活性表现：**而最终的费率决策取决于效用函数的权重，使用面向吞吐量的效用函数，将获得更高的效用值，而使用面向延迟的效用函数，将获得更高的效用值。这样，Libra就可以根据应用程序的偏好细粒度地调整性能偏好，显示出良好的灵活性

我们形成了五个不同的效用函数:default、Th-1 (2x默认的时延)、Th-2 (3x默认的时延)、La-1 (2x默认的时延)、La-2 (3x默认的时延)，并使用图7中相同的仿真网络来测试它们的性能。如图11(a)和图11(b)所示，Libra可以获得预期的性能偏好，满足多样化的**应用需求。在使用面向吞吐量的效用函数时，Libra的吞吐量比BBR高，而在使用延迟感知效用函数时，Libra的延迟比Orca低。这样，Libra就可以根据应用程序的偏好细粒度地调整性能偏好，显示出良好的灵活性**

5.3实用性表现：开销，公平性，收敛特性，提供安全保证的能力

5.4网络直播的表现

5.5深入了解改进：每个候选率的应用次数占比，比较libra和理想的组合版本

### related work

经典CCA，基于学习的CCA，组合CCA：Rein[9]根据运行时环境中途切换不同的cca。

### discussion

如何选择libra的参数，如果我们将Libra应用于其他网络会怎么样?

### conclusion

在本文中，我们提出了Libra，这是一个组合拥塞控制框架，以补充经典cca和基于学习的cca的优势。Libra可以适应多种网络条件，并根据应用需求调整性能偏好。在互联网直播和Pantheon上进行的大量实验阐明了Libra组合机制性能提升的来源，并证明Libra可以实现我们设计的适应性、灵活性和实用性的目标。

## One Rein to Rule Them All: A Framework for Datacenter-to-User Congestion Control（一个控制它们的规则:数据中心到用户拥塞控制的框架）

[APNet '20: Proceedings of the 4th Asia-Pacific Workshop on Networking](https://dl.acm.org/doi/proceedings/10.1145/3411029)August 2020Pages 44–51

### abstract

在这项工作中，我们提出了一个新的网络拥塞控制框架Rein。使用Rein，不同的拥塞控制算法可以有目的地分配到一个服务器上的连接，以适应异构性。

### introduction

我们采取了另一种方法来提高异构环境下拥塞控制的性能:根据观察到的每个连接的网络特征，为每个连接选择合适的CCA。我们将性能下降归咎于刚性单一CCA策略与网络多样性之间的不匹配，我们试图通过识别连接的网络特征和正确选择合适的CCA来弥合差距。本文介绍了一个网络拥塞控制框架Rein，它有两个主要功能:收集数据以表征连接和切换CCA以适应异构。

### motivation and basic idea

2.1考虑异质性的必要性： 数据中心中的服务器所面对的客户端，其连接具有多种网络特征。一个CCA无法在不同的场景中脱颖而出，因为它的设计假设可能会被复杂的现实所违背。

2.2为什么选择CCA：现有的旨在使拥塞控制适应异构的工作存在实际问题。选择CCA是另一个框架，由于以下原因，它可以有效地使服务器端拥塞控制适应异构性。

2.3基本思路：为了提高整体传输性能，我们打算为每个连接选择合适的CCA，以便CCA分别匹配每个连接的网络特性。我们对每个连接采取以下措施。**(1)设置初始CCA，从连接的前端数据包序列中采集网络反馈数据(如带宽、RTT等);(2)采集数据表征连接，并将其CCA切换到匹配的算法。相应的，Rein是一个不需要修改接收器的框架，它提供了两个功能:(1)在网络协议栈中有效地暴露网络反馈数据;(2)根据用户自定义的选择策略，在线顺利切换到所选的CCA。**

2.4挑战：系统挑战，政策挑战：从本质上讲，选择CCA就是预测每个候选CCA在给定连接上的性能，并采用最胜任的CCA。

### design

包括两个关键模块:Selector和Agent，它们通过一对管道交换信息。**Agent负责在网络协议栈中收集数据并交换CCA, Selector负责将收集到的数据映射到适合的CCA。**工作流程:Agent收集数据并通过上行管道向Selector公开，Selector将数据映射到相应的CCA，并通过下行管道向Agent发送CCA切换通知，Agent解析通知并在网络协议栈中进行算法切换。

#### 3.1Agent

Agent通过检测和重构网络协议栈来实现数据采集和算法切换。**数据收集。**数据收集功能在ACK处理过程中进行检测，在收到ACK时调用。首先，Agent提取信息(例如:(**当前RTT，损失率，带宽)**，然后将它们写入向上管道，然后由选择器读取数据。**算法切换。**算法切换通过动态替换CCA和保证CCA平滑过渡两种措施实现。

#### 3.2 Selector

选择器的主要功能是将收集到的数据映射到匹配的CCA，这是今后工作的重点，我们简要描述了关键思想。我们采用两种方法来选择匹配的CCA。(1)**首先检查该连接是否面对典型网络场景，如果是，则选择相应的专用CCA。**我们之所以采用这种方法，是因为从收集的数据中提取具有明确物理意义的特征，可以直接识别典型案例。例如，可以识别典型的无线连接(§4.2.2)。对于这些情况，可以应用人工设计的规则将数据映射到CCA，我们称之为基于规则的选择[5]。(**2)对于一般情况，我们打算应用数据驱动方法来获得映射，基于我们的CDN数据集中捕获的两个初始观测值。**因此，我们可以通过参考过去**具有相似multivariate time series （MTS，相似多变量时间序列）的连接的CCA性能**来选择新的到达连接的CCA。

(2)对于一般情况，我们打算应用数据驱动方法来获得映射，基于我们的CDN数据集中捕获的两个初始观测值。(a)由于网络的时空局部性，对于大多数连接，存在其他具有类似MTS的连接(图6)。(b)具有相似MTS的连接往往表现出相似的CCA性能。我们发现MTS在所有属性(RTT，带宽…)上匹配的连接往往面临同质网络环境，并且这些连接的CCA性能差异相对稳定，例如在该环境下，BBR在大多数情况下优于CUBIC。因此，我们可以通过参考过去具有相似MTS的连接的CCA性能来选择新的到达连接的CCA。**我们计划在MTS数据中编码模式，并通过数据驱动的方法挖掘MTS模式与CCA性能之间的关系，我们将这种方法称为基于学习的选择方法。**我们在§4.2.2中提供了一个选择策略实例来演示选择器工作流。

4.2.2选择器。选择器目前只实现基于规则的选择，我们提供了一个区分WiFi连接和有线连接的实例来演示工作流程，分类的准确性和更丰富的规则将在未来的工作中进一步探索。通过对连接轨迹的分析总结出这些规律，发现WiFi环境下RTT的抖动非常剧烈，我们通过采样RTT的变异系数(COV，标准差除以均值)和归一化范围((最大值-最小值)/最小值)对其进行量化。该规则描述如下:如果一个连接的COV和采样rtt的归一化范围在前N个采样rtt中超过一定的阈值(伪代码中的CTH和RTH)，我们推测该连接是通过WiFi连接，并为该连接设置TCP Westwood。

### implementation

#### 4.1内核实现

Rein在内核中包含两个部分:作为外部内核模块实现的Pipe和作为TCP栈修改实现的Agent。我们测量了ACK处理过程以生成原始数据，并提取每个ACK信息，例如RTT，损失率，交付率，ECE (ECN-Echo)标记等。**许多cca以可加载模块**的形式在Linux内核中实现。改变连接的cca可以通过**替换函数指针**来实现。

#### 4.2用户空间实现

通过对连接轨迹的分析总结出这些规律，发现WiFi环境下RTT的抖动非常剧烈，我们通过采样RTT的变异系数(COV，标准差除以均值)和归一化范围((最大值-最小值)/最小值)对其进行量化。**该规则描述如下:如果一个连接的COV和采样rtt的归一化范围在前N个采样rtt中超过一定的阈值(伪代码中的CTH和RTH)，我们推测该连接是通过WiFi连接，并为该连接设置TCP Westwood。**

### evaluation

5.1算法切换：Rein可以实现平滑在线的算法切换。其中涵盖了主流算法类型，即基于速率/基于窗口，基于延迟/基于损失。

**5.2提高性能：**因为BBR相对于小缓冲区场景下基于损失的CCA具有优势[13,21]。

5.3管理开销：结果表明，Rein引入的开销是适度的，而netlink的成本是不可忽略的。

### related work

Rein的控制单元是CCA。Rein的目标是利用现实世界的多变量时间序列网络反馈数据。细粒度的可配置网络协议栈。**通过配置协议参数，使网络协议栈可定制以适应客户端的特性**，这方面已经出现了激增。

### conclusion

我们认为，对于异构网络环境下的连接，不应采用单一的拥塞控制算法，而应根据每个连接的特点选择更优的算法。我们设计并实现了Rein的原型来支持我们的论点，它旨在通过利用客户网络的异质性来提高性能。初步评估证实了Rein的可行性。未来的工作将集中在设计CCA选择策略和在我们的CDN(content delivery network，内容分发网络) 服务器中部署Rein。

## Reducing Mobile Web Latency Through Adaptively Selecting Transport Protocol（通过自适应选择传输协议降低移动Web延迟）

**Published in:** [IEEE/ACM Transactions on Networking](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=90) ( Volume: 31, [Issue: 5](https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=10286433&punumber=90), October 2023)

### abstract

在本文中，我们提出了一种自适应**传输协议选择机制WiseTrans**，用于在线切换移动web服务的传输协议，并提高web请求的完成时间。WiseTrans引入机器学习技术来处理时间异质性，利用历史信息做出决策来处理空间异质性，采用在线学习方法来跟上实现变化的步伐，并在请求级别切换传输协议以在可接受的开销下达到高性能。（TCP与QUIC之间切换）

### introduction

因此，我们有动力采用适当的传输协议以获得更好的性能。本文提出了一种**自适应传输协议选择机制WiseTrans**，以提高移动web服务的性能。WiseTrans在客户端测量网络状况，并在必要时切换传输协议。我们提出了一种基于XGBoost的自适应传输协议选择机制WiseTrans，用于大规模部署移动web服务(§III)。

来自不同用户的连接将穿越具有不同行为的广域网和移动接入网(即空间异质性)。

我们在WiseTrans中设计了几个构建块。首先，为了适应移动用户的网络时间波动，WiseTrans定期测量网络状况，并决定对即将到来的请求是使用QUIC还是TCP。其次，为了对异构用户进行优化，WiseTrans收集历史决策和性能，然后随后为不同用户在线更新和定制协议决策(§III-B)。第三，为了使WiseTrans可推广到各种协议变体，我们引入了一个概率探索机制来更新TCP和QUIC的行为(§III-D)。最后，为了使WiseTrans能够以最小的开销部署到现实世界的大规模服务中，同时保持性能改进，我们做出了一系列的设计选择，包括使用基于树的机器学习算法进行决策(§III-C)，并仔细设计决策粒度(§III-A)。

### background and motivation

我们首先介绍§II-A中用于移动web服务的QUIC和TCP之间的性能差异。然后，我们在§II-B中提出了移动web服务自适应协议交换的挑战。

背景：QUIC的统计平均结果在慢网(高丢包率、低带宽)条件下优于TCP。对RTT的测量也显示了类似的结果。正如最近的进展所衡量的那样，在这些慢速网络中，QUIC的更高性能归因于其更好的丢失检测和恢复，以及消除了Head-Of-Line (HOL)阻塞和ACK歧义[5]，[6]，[7]，[11]。

挑战：复时间相关，空间异质性高，实现多元化，移动设备上有限的资源

WiseTrans的设计目标是:(1)具有足够的表达能力，以反映复杂的网络条件;(2)具有足够的自适应能力，以应对由用户异构性引起的不可预测的协议性能变化;(3)不同实现之间的泛化能力强;(4)重量轻，带来的额外开销少。

### wisetrans design

#### A设计概述

首先，为了对异构用户进行优化，Feature Extractor不仅收集历史网络条件，还收集历史决策及其性能，然后随后为不同用户在线更新和定制协议决策(§III-B)。其次，为了处理连接过程中复杂的时间关系，同时保证大规模部署的简单性，Network Monitor采用了基于树的分类算法XGBoost[19]来监控网络状况(§III-C)。第三，为了使WiseTrans适用于各种协议变体和应用程序实现，Probe Agent探索了TCP和QUIC的实时行为(§III-D)。最后，我们介绍了回滚检查器作为网络意外行为和协议意外决策的保护(§III-E)。

具体来说，**WiseTrans作为应用层和传输层之间的缓冲层**。WiseTrans从应用层和传输层收集信息，做出决策，并允许应用程序采用所选择的协议。

WiseTrans的工作流程如下：步骤1:特征提取。步骤2:网络监控。步骤3.1:协议探索。步骤3.2:回滚检查，步骤4:协议切换。

WiseTrans在请求级工作，主要考虑以下因素：移动设备的开销。请求的及时性。协议切换一致性。

#### B. Feature Extractor

Feature Extractor收集和提取用于协议选择的特征。

历史网络状况。我们使用往返时间(RT T)、瓶颈带宽(BtlBw)和丢包率(LossRate)来建模网络路径，如下[22]，它可以描述用户所经历的网络条件。基于§III-A中提到的设计选择，我们收集请求级平均RTT (RT T_avg)，吞吐量(resp_rec_G)和重传速率(Retran_rate)作为网络条件。

历史决定。还应该考虑传输协议。历史决策及其后续性能可以解决决策边界的异质性和可变性。对于某些用户，QUIC/UDP可能的速率限制可以体现在网络条件相似，协议相同但性能不同的情况下。这样，就可以探讨决策边界的变化。

历史请求的性能。我们选择Time to First Byte (T T F B)并请求completion goodput (req_cmpl_G)作为过去决策的性能。如图8所示，本文的TTFB是客户端发送请求到客户端接收第一个字节的时间，它暗示了请求传输的属性。我们收集历史平均TTFB (T TF B_avg)来估计应用程序性能。我们还计算了当前请求的TTFB与平均TTFB的比率(T TF B_rate)，旨在捕捉瞬时性能变化。req_cmpl_G度量请求级的good - put，将用户接收到的字节数除以客户机发送请求和接收整个响应体之间的时间间隔。

对于每个请求，Feature Extractor记录请求所需的信息，以及HTTP日志和套接字日志中的连接信息。

#### C. Network Monitor

我们在Network Monitor中建立了一个模型来检测网络条件的变化，首先，网络状况变化程度的标准应该与TCP和QUIC的性能密切相关。其次，使用特定的QUIC和TCP实现训练的模型，即使在不同的实现中，仍然可以用来理解网络条件的变化。我们离线训练模型以反映网络变化的程度。Network Monitor应该能够根据Feature Extractor中的特征忠实地学习到时间相关性和空间异质性。潜在的模型包括线性回归、支持向量机、决策树、随机森林，甚至神经网络。在本文中，WiseTrans采用了基于树的XGBoost模型。

**Network Monitor给出一个反映当前网络状况的指标，该指标的波动将作为Probe Agent进入探测阶段的条件**。为此，该指标应该根据网络条件对TCP和QUIC性能的影响对网络条件进行评分。

对于XGBoost的训练，WiseTrans离线学习模型。根据使用TCP和QUIC的性能比较，对特征提取器中的特征进行标记。（涉及机器学习，跳过）

#### D. Probe Agent

Probe Agent进行现场实验，比较TCP和QUIC的性能，选择后续协议。探测代理包括两个阶段:探测阶段和开发阶段。在探索阶段，Probe Agent通过交替使用TCP和QUIC发送请求来进行实时实验，并比较性能。在开发阶段将使用适当的协议。

Probe Agent通过交替使用TCP和QUIC进行现场实验，Probe Agent收到这些请求的响应后，比较性能，选择更好的协议，为了获得准确及时的测量结果，我们交替使用TCP和QUIC各三次。我们对每个协议使用三个请求的两个相似的性能值，并选择平均性能较好的协议，探测阶段在探测代理接收到带有QUIC的第三个请求的响应时结束。在探索阶段的第三个请求之后发送的请求仍将使用最后选择的协议。注意，Probe Agent通过应用程序的实际请求执行实时实验，不会偶尔发送探针或使用一次性数据进行测量。当最优协议更有可能发生变化时，即以下两种情况，Probe Agent应该进入探索阶段:建立新的联系。网络条件变化很大，我们使用network Monitor评分的网络状况，并保留上次探索阶段后的网络状况评分作为历史评分。**当当前网络评分与历史评分的差值超过我们设置的阈值时**，Probe Agent进入探测阶段。ISP（互联网服务提供商的策略）

#### E. Rollback Checker

我们引入了回滚检查器来保护网络的意外行为和协议的意外决策。在大多数情况下，由网络监视器和探测代理决定的协议是有效的，但也有一些情况下，网络、服务器或用户的意外行为可能会破坏结论。首先，网络条件的突然波动会降低探测代理的有效性。此外，对于Network Monitor和Probe Agent来说，捕获可能的UDP/QUIC块和速率限制并对即时变化做出反应是一项挑战，如图7所示。此外，如前所述，协议选择可能会受到网络和服务器端不可预测的外部事件的影响，例如意外的路由错误或服务器的突然过载。因此，需要Rollback Checker来处理错误的决策。

回滚检查程序在发生协议切换时开始工作。鉴于上述情况，**Rollback Checker在切换前后的一段时间内比较利用阶段发送的请求的性能。如果发现明显的性能下降，回滚检查器回滚到最后一个协议。**

### evaluation

在本节中，我们首先介绍我们在评估中使用的基线算法(§IV-A)，然后是评估的设置(§IV-B)。我们从以下几个方面对WiseTrans进行评估：野生性能，概况分析，组件的有效性

A基线算法

B.实验设置：因此，WiseTrans优化的连接是指移动用户与前端服务器之间的连接

C.请求完成时间：这样的结果体现了WiseTrans的核心思想，即在当前网络条件下选择性能更好的协议。具体来说，WiseTrans应该始终如一地实现使用QUIC或TCP的最佳性能。

D.概化分析：WiseTrans推广到其他实现的能力，在实践中，WiseTrans-v2可能能够推广到其应用程序采用的广泛实现和协议变体。

E. WiseTrans Deep Dive：我们将评估WiseTrans设计的有效性，并提供更深入的理解。我们首先通过比较WiseTrans和朴素的strawman方法来说明使用机器学习方法的必要性。然后我们评估了网络监视器中使用的模型的性能，并评估了它的训练过程。然后，我们证明了现场实验的频率和有效性。最后，通过实验验证了回滚检查器的有效性。

### related work

TCP和QUIC的性能测量。QUIC实现的多样性。运输参数的自适应优化：传统上，研究者设计了不同的机制来动态调节CWND以适应网络条件[46]，[47]，[48]。随后，近期研究者提出对现有协议的配置进行动态调整[49]、[50]、[51]、[52]，以进一步扩大适应范围。网络预测的机器学习：WiseTrans在请求级别离散化，不仅考虑网络条件，还考虑先前的决策和后果。对早期协议选择工作的增强。

### discussion

与自适应传输参数机制的集成：WiseTrans可以集成TCP变体选择机制[57]，[58]或在线配置TCP参数[51]，[52]。

互联网传输协议的未来：这意味着自适应地使用TCP和QUIC至少可以是一种增量部署，并被业界采用。

泛化分析的局限性：我们的评估仅验证了WiseTrans在两个类似实现中的泛化能力。需要进一步分析不同实现之间的泛化性能。

### conclusion

我们提出了WiseTrans，这是第一个以优化方式自适应选择传输协议并在现实世界中部署的解决方案。

## PCC Proteus: Scavenger Transport And Beyond

[SIGCOMM '20: Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication](https://dl.acm.org/doi/proceedings/10.1145/3387514)July 2020Pages 615–631

### abstract

许多Internet应用程序需要高带宽，但对时间不敏感。这就激发了拥塞控制“清道夫”，它主动让位给优先级更高的应用程序，从而改善了整体用户体验。然而，现有的清除协议LEDBAT常常不能产生结果，存在性能缺陷，并且需要与其他传输协议分离的代码库。我们提出了PCC Proteus，一个新的拥塞控制器，可以作为一个有效的清道夫或主协议。我们提出了PCC Proteus，一个新的拥塞控制器，可以作为一个有效的清道夫或主协议。Proteus采用了一些新颖的想法，以确保它在获得高性能的同时屈服于主要流，包括使用延迟偏差作为竞争信号，以及动态环境下的噪声容忍技术。通过扩展现有的PCC实用程序框架，Proteus还允许应用程序指定灵活的效用函数。除了清道夫和主要模式，**允许选择两者之间的混合模式，更好地捕获应用程序需求**。**Proteus显著改善了页面加载时间和DASH视频传输，**它的混合模式显著减少了带宽受限环境下的再缓冲。

### introduction

**当用户需求是异构的时候，平等地共享资源通常不是最优的**。传统的拥塞控制，在一个共同的瓶颈上平均分配带宽，可能导致较低的网络利用率。在当今使用网络的各种应用程序中，至少有一些流程具有类似的弹性资源需求:软件更新，在线数据备份，云存储的后台复制(例如Dropbox)， cdn中的主动缓存预热，以及用于离线分析的物联网传感器数据聚合等。这些应用程序可能占用对其用户来说过多的带宽，并且可能被更多数据密集型应用程序所消耗。

即使是那些通常对时间敏感的应用程序有时也会变得灵活。当机器学习任务受到慢工的阻碍时，接收其下一阶段工作的输入的优先级可能较低。**通常具有弹性需求的应用程序有时可能要求增加优先级，标准拥塞控制协议不能适应这种上下文敏感的优先级。**当然，这种方法不会像集中式资源分配器那样接近最优，但它的可部署性使其成为通用互联网拥塞控制的实用方法，即在端到端传输中

我们的解决方案PCC Proteus通过以下设计贡献扩展了PCC中基于效用函数的方法[16,17]:

贡献：1.我们为初级(Proteus-P)和清道夫(Proteus-S)发送者建立了效用函数目标。**清道夫效用函数采用基于延迟偏差的惩罚**，这提供了一个敏感的竞争信号，通常不会被主要流使用。

2.-h **具有分段效用函数，可根据跨层应用需求(例如，在线视频的最大比特率)确定自适应阈值，在主模式和清道夫模式之间切换。**

3.-s我们介绍了更好地响应网络延迟噪声的技术(即，与信道相关的端到端延迟中的非拥塞可变性，而不是与发送方选择的速率相关)，以便清除器可以在无线网络等高度动态环境中实现稳健的性能。

### preliminaries and motivation

2.1什么时候清道夫有意义？

通常，我们认为清除在以下情况下是有效的:(1)清除程序对时间不敏感，成本可以忽略不计;(2)选择使用清除程序的应用程序设计人员有可能获益(可能是间接地)。例如，移动电话制造商可能会为自动软件更新选择清除程序。一些大型云提供商提供从云存储到视频传输的多种流行服务

2.2信号通知清道夫让出带宽

(1)同样的指标，更大的惩罚：清道夫可以采用与感兴趣的主要协议相同的指标，但代价更大，因此它更保守。

(2)不同的指标。清道夫会以某种方式从不同于主要协议的度量中获取信号。

2.3灵活性的动机（混合模式的动机）

因此，如果有一个灵活的、通用的互联网拥塞控制体系结构，它将综合主模式和清道夫模式，并简化协议内和协议间交互的形式分析，即清道夫与清道夫，清道夫与主流。

### proteus design overview

Proteus将拥塞控制分为一个实用模块和一个速率控制模块。

实用模块具有实用函数库，可以根据不同应用程序的需要进行定制。

我们可以基于选择的性能指标(§2.2)构建效用函数，同时允许这些效用函数共享相同的速率控制算法(例如，PCC Vivace[17]中基于梯度的速率控制)。

首先，我们为清道夫发送者设计了一个新的实用函数，称为Proteus-S，它通过利用延迟偏差作为流量竞争的信号来满足我们的产量和性能目标。其次，为了满足灵活性目标，**Proteus系统支持动态效用函数选择。应用程序可以实时地选择或重新选择实用程序功能，甚至在流的中间也是如此。**除了Proteus-S之外，应用程序还可以在称为Proteus-P的初级流实用函数和称为Proteus-H的新混合模式实用函数中进行选择，它结合了Proteus-S和Proteus-P，根据应用程序的吞吐量需求，以自适应的分段功能。

首先，我们设计了新的实用函数，特别是清道夫及其扩展到混合动力模式(§4)。其次，由于清道夫效用函数对非拥塞RTT噪声敏感，我们设计了新的耐噪控制机制(§5)。

### protesus utility design

在介绍了主协议模式之后，我们讨论了我们的清道夫使用的关键指标，然后讨论了Proteus-S的清道夫实用函数。最后，我们将Proteus-P和Proteus-S组合成一个混合模式(Proteus-H)，采用分段效用函数和跨层设计，以改善带宽分配。

#### 4.1primary模式的效用函数

我们使用PCC Vivace效用函数[17]，稍作修改—忽略负RTT梯度:上述Proteus-P效用函数是延迟感知的，并惩罚两个性能指标:RTT梯度和丢包率。其收敛性由[6]中证明的三个常数参数决定。在Proteus-P中，我们使用与[17]中相同的默认值(𝑡= 0.9，𝑏= 900，𝑐= 11.35)。

#### 4.2竞争指标（使用清道夫的指标）：RTT偏差

我们选择RTT偏差作为流量竞争的指标。RTT偏差是一个MI内RTT样本的标准差，计算为

RTT偏差捕获延迟，从而缓冲占用动态，引起流量竞争。这证实了RTT偏差提供了一个更敏感的RTT波动动态的早期信号，

然而，延迟噪声(即非拥塞RTT可变性)也可能在某些网络中导致偏差，比如快速变化的无线网络。因此，我们设计了噪声控制机制(§5)来增强鲁棒性。

#### 4.3scavenger模式的效用函数

当Proteus-S和Proteus-S发送者相互竞争时，公平；当Proteus-P和Proteus-S发送者相互竞争时，我们将Proteus-S发送端为Proteus-P发送端提供带宽的正式分析留给未来的工作。**由于Proteus-S效用函数中的RTT偏差项产生较大的惩罚，使得Proteus-S发送方相对保守，因此Proteus-S发送方向Proteus-P发送方屈服。当主要协议不是Proteus-P时，RTT偏差的有效性可以通过§4.2非正式地证明，并通过我们的实验验证。**

#### 4.4proteus-H混合模式

Proteus-H基于阈值在清道夫和主模式之间切换。但在控制算法中没有显式的开关;它只是通过比较不同发送速率的效用值而隐式地发生。当两个Proteus-H发送端具有切换阈值𝑟1和𝑟2时(𝑟1 <𝑟2)在瓶颈上与容量C竞争，我们期望它们收敛于速率对(x1，x2)，其中:

(𝑟1 <𝑟2)在瓶颈上与容量C竞争，我们期望它们收敛于速率对(≥1，≥2)，其中:

交换阈值的跨层设计。应用程序应该自适应地设置𝑢𝐻()中的阈值。我们为视频流开发了一个阈值策略。我们从视频比特率适应的三个观察开始:(1)只要流畅地呈现最高的视频质量，用户就不会注意传输吞吐量。

(2)客户端只在本地播放缓冲区有足够空间时才会请求下一个视频块。

(3)当视频在重新缓冲时停止时，客户端希望尽可能大的吞吐量来恢复。

对于比特率适应，我们可以动态地将阈值设置为满足以下两个规则的最大值:

(1)充分速率规则:阈值≤𝐺·bitratemax。我们设置𝐺= 1.5，这样就有足够的安全余量来避免重新缓冲。

(2)缓冲区限制规则:阈值≤1 2−𝑓·bitratcurrent，其中𝑓是缓冲区中可用空间块的数量(可能是小数)。此规则适用于𝑓< 2，并在请求新块时进行检查。其结果是，当缓冲区接近满时，阈值将降低(因此没有必要快速加载块，因为无论如何，如果缓冲区满了，ABR算法将暂停传输)。

然后，每当重新缓冲发生时，以下规则将覆盖切换阈值，直到视频恢复。

(3)紧急规则:阈值=∞。

我们的实验表明，当使用基于缓冲区的自适应算法(如BOLA[35])时，上述规则有效地提高了整个网络的效率**。我们应该注意的是，我们将此作为基准测试的代表性解决方案;它可能不适合使用吞吐量进行控制的比特率自适应**。我们把Proteus-H整合到其他视频流算法和其他类型的应用程序中，留给未来的工作。

### 处理延迟噪声

#### 每ack: RTT样本过滤。

我们使用“ACK间隔”(接收到两个连续ACK的时间间隔)来过滤掉异常的RTT样本，当两个连续的ACK间隔之间的比率超过阈值时(在我们的实现中设置为50)。

#### Per-MI:回归容错。

我们计算其估计回归RTT为:

然后根据线性回归中的残差计算回归误差:

#### MI历史:趋势容忍。

为了避免这种延迟反应，Proteus会在较长时间内跟踪与延迟相关的指标。具体来说，发送者维护最近𝑘MIs的RTT偏差和平均RTT(例如，在我们的实验中𝑘= 6，以便在噪声脆弱性和缓慢响应之间进行合理权衡)，基于此，它计算两个趋势指标:趋势梯度和趋势偏差.然后，对于两个指标的每个新样本，我们将其与相应的平均值进行比较。我们的见解是，**当计算的趋势度量样本离其平均值有几个偏差时，统计上不太可能是由非拥挤噪声引起的，因此不能忽略**。我们用下面的伪代码来说明：

具体来说，如果更新的趋势梯度与其移动平均值(avg_trend_grad)之间的差异小于𝐺1乘以趋势梯度的偏差(dev_trend_grad)，则新的RTT梯度样本将被忽略。在这种情况下，如果趋势偏差与其移动平均(avg_trend_dev)之间的差异小于𝐺2乘以其偏差(dev_trend_dev)，则RTT偏差也被忽略。在我们的实现中，我们保守地选择𝐺1 = 2和𝐺2 = 4，以在正态分布的延迟噪声下大约达到95%以上的置信度

#### 控制算法:多数决定原则。

对于处于“探测”状态的每个速率控制决策，Vivace尝试一对发送速率(以随机顺序)两次，并且仅当它们暗示一致的速率变化方向[16]时才更改发送速率。在高度嘈杂的环境中，这可能会导致速率上升缓慢和利用率不足，在这种环境中，发送者看到更多不一致的速率变化指标，并且必须在正确提高速率之前反复测试同一对发送速率。

**我们让Proteus发送者尝试每对发送速率三次(而不是两次)，并根据三对试验的多数决定改变发送速率。**通过增加额外的对，在有噪声的网络中，发送方通常可以更快地确定速率变化的方向，而多数规则有效地避免了频繁的错误速率变化方向。

#### 注意

简而言之，即使在相对稳定的瓶颈上，对于Proteus发送方来说，每mi回归容错是饱和带宽所必需的，而趋势容错有助于增强延迟敏感性。RTT样本过滤机制和多数规则在速率控制中的使用主要使Proteus在高动态网络中受益，这可以在一定程度上通过Proteus在实时互联网上优于Vivace的性能改进来证明(§6.2.1)。然而，我们强调，上述容忍机制是启发式的，并没有理论上的性能保证。当网络延迟噪声在MI的时间尺度上出现非常突然时(在我们的真实WiFi测试中观察到§6.2.1)，性能仍然可能受到影响，导致RTT梯度和RTT偏差的异常样本。

### 评估

我们比较了两种清除器——Proteus-S和LEDBAT，并让它们与各种主要协议(TCP CUBIC[21]、BBR[13]、COPA[8]、PCC-Vivace[17]和Proteus-P)竞争。

#### 6.1Scavenger-Only性能

#### 6.2屈服于主流

#### 6.3混合动力的灵活性

### 讨论

#### 7.1现实世界的应用

#### 7.2噪声容忍的鲁棒性

### 相关工作

### 结论

我们提出了PCC Proteus，一个网络拥塞控制架构。Proteus支持从应用程序到传输层的交互，根据应用程序的需求定制拥塞控制，特别是实现拥塞控制清除器。具体而言，基于在线学习实用程序框架[16]，我们设计了一个协议，该协议既可以作为主协议(Proteus-P)，也可以使用专用的清道夫实用程序函数作为清道夫(Proteus-S)。通过在仿真网络和实时互联网上的综合实验，我们证明了Proteus-S作为一种清除者对各种竞争协议的鲁棒性。我们还将Proteus扩展为一个混合的清道夫/主设计，它实现了更高的应用级实用程序，用于自适应比特率视频传输和网页加载，并展示了我们方法的灵活性。我们相信这条研究路线对于处理必须在高优先级和更具弹性时间要求的流量之间共享受限带宽的互联网环境将变得越来越重要。

## NSDI18 Copa- Practical Delay-Based Congestion Control for the Internet

Open Access Media：USENIX

### 摘要

本文介绍了端到端拥塞控制算法Copa，它采用了三个思想。首先，它表明目标速率等于1=(ddq)，其中dq是(测量的)排队延迟，在马尔可夫包到达模型下优化吞吐量和延迟的自然函数。其次，它根据目标速率的方向调整其拥塞窗口，即使面对显著的流量波动，也能迅速收敛到正确的公平速率。这两个想法使一组Copa流能够以低排队延迟保持高利用率。

然而，当瓶颈与基于损失的拥塞控制流共享时，Copa与其他延迟敏感方案一样，实现了低吞吐量。为了解决这个问题，Copa采用了第三种方法:通过观察延迟演化来检测缓冲填充器的存在;用d参数的加性增加/乘性减少来响应。

### 介绍

我们问是否有可能开发一种拥塞控制算法，以实现高吞吐量、低排队延迟和公平速率分配的目标，但它也很容易理解，并且在广泛的环境和工作负载中具有普遍的适用性，并且至少与为特定情况设计的最佳先前方案一样好。

我们表明，在某些简化的(但合理的)数据包到达的建模假设下，使U最大化的稳态发送速率(以每秒数据包为单位)为

### copa算法

Copa包含三个思想:第一，目标率，它与测量的排队延迟成反比;第二，一个窗口更新规则，依赖于移动发送者的目标率;第三，tcp竞争策略，以更好地与缓冲区填充流竞争。

#### 2.1目标速率和更新规则

速度参数v，加速收敛。

初始化为1。每个窗口一次，发送方将当前cwnd与发送最新确认的数据包时的cwnd值(即当前窗口开始时的cwnd)进行比较。

如果当前cwnd较大，则将方向设置为“向上”;如果它较小，则将方向设置为“\down”。

现在，如果方向与前一个窗口相同，则加倍v，如果不是，则将v重置为1。然而，只有在三个rtt的方向保持不变后才开始加倍v。**稳态时，我们希望v=1。**

#### 2.2与缓冲填充方案的竞争

我们为Copa提出了两种不同的运作模式:Copa根据是否检测到竞争的长时间缓冲区填充方案在这些模式之间切换。

1.默认模式，其中d =0:5，

2。一种竞争模式，其中动态调整d以匹配典型缓冲区填充方案的侵略性。

检测器利用一个关键的Copa属性，即当只有具有相似RTT的Copa流共享瓶颈时，队列至少每5·RTT空一次(第3节)。即使只有一个并发的长时间缓冲区填充流，队列也不会在这个周期内清空。因此，如果发送方在最后5个rtt中看到一个“几乎为空”的队列，它将保持默认模式;否则，它将切换到竞争模式。

### 3动态copa

所谓“均衡”，我们指的是每个发送者都以其目标速率发送信息的情况。讲copa如何达到稳态

### 4Copa目标速率的合理性

本节解释Copa中使用的target rate的基本原理。我们对瓶颈处的数据包到达进行建模，不像前一节中那样是确定的到达，而是泊松到达。

#### 4.1目标函数与纳什均衡

#### 4.2 Copa更新规则由均衡速率推导

每个发送者不需要知道有多少其他发送者，也不需要知道他们的偏好是什么。此分析的目的是为sender确定一个良好的target rate。

#### 4.3平衡的性能

对这个均衡做一些评价。

第四，平衡点的定义与我们的更新规则是一致的，因为当(且仅当)系统处于纳什均衡时，每个发送者的传输速率等于其目标速率。

### 5评估

为了评估Copa并将其与其他拥塞控制协议进行比较，我们使用了用户空间实现和ns-2模拟。我们在模拟链接和真实链接上运行用户空间实现。

#### 5.1仿真链路的动态行为

#### 5.2真实世界评价

#### 5.3rtt公平

#### 5.4对丢包的鲁棒性

#### 5.5模拟数据中心组网

#### 5.6模拟卫星链路

#### 5.7与缓冲填充方案共存

### 6相关工作

第三，BBR和Copa都寻求定期清空它们的队列，以正确估计传播延迟。

### 7结论

​	我们描述了Copa的设计和评估，Copa是一种实用的基于延迟的互联网拥塞控制算法。其思想是增加或减少拥塞窗口取决于当前速率是否低于或高于定义良好的目标速率，1=(ddq)，其中dq是(测量的)排队延迟。我们展示了这个目标速率如何优化吞吐量和延迟的自然函数。Copa使用一个简单的更新规则在目标速率的方向上调整拥塞窗口，即使面对显著的流量波动，也能快速收敛到正确的公平速率。

​	这两个想法使Copa流能够以低排队延迟保持高利用率(平均而言，队列中每个流的数据包为1:25=d)。然而，当瓶颈与像Cubic或NewReno这样的缓冲区填充流共享时，Copa和其他延迟敏感方案一样具有低吞吐量。为了解决这个问题，Copa发送方通过观察延迟演变来检测缓冲区填充器的存在，然后在d参数上响应AIMD以更好地与这些方案竞争。

## NSDI20 ABC A Simple Explicit Congestion Controller for Wireless Networks

Open Access Media：USENIX

### 摘要

我们提出了Accel-Brake Control (ABC)，这是一种简单且可部署的明确拥塞控制协议，适用于具有时变无线链路的网络路径。**ABC路由器用“加速”或“刹车”标记每个数据包，这使得发送者稍微增加或减少他们的拥塞窗口。**路由器使用这种反馈来快速引导发送者达到期望的目标速率。ABC不需要更改报头格式或用户设备，但性能优于XCP。ABC也是可增量部署的;当瓶颈是非abc路由器时，它可以正常工作，并且可以与非abc流量共存，共享同一条瓶颈链路。我们使用Wi-Fi实现和跟踪驱动的蜂窝链路仿真来评估ABC。在相同的延迟下，ABC的吞吐量比Cubic+Codel高30-40%，比Wi-Fi路径上的BBR延迟低2.2倍。在蜂窝网络路径上，ABC的吞吐量比Cubic+Codel高50%。

### 1介绍

然而，当前的显式控制协议有两个限制，一个是概念上的，另一个是实际的。首先，现有的显式协议是为固定容量链路设计的;我们发现它们的控制算法在时变无线链路上是次优的。其次，它们需要对在Internet上部署的数据包头、路由器和端点进行重大更改。我们的贡献是一个简单且可部署的协议，称为Accel-Brake Control (ABC)，它克服了这些限制，建立在先前的立场文件[22]的概念之上。在ABC(§3)中，无线路由器根据对当前链路速率的测量估计，用与加速或制动相对应的一位反馈标记每个数据包，在通过接收方的ACK接收到这个反馈后，发送方在加速时将其窗口增加1(发送两个数据包作为对ACK的响应)，在制动时将其窗口减少1(不发送任何数据包)。这个简单的机制允许路由器在一个RTT内发出一个大的动态窗口大小变化范围的信号:从将窗口限制为0到将窗口增加一倍。

ABC性能的核心是一种新的控制算法，它可以帮助路由器对时变链路提供非常准确的反馈。现有的显式方案，如XCP和RCP，通过比较当前数据包的排队率和链路容量来计算反馈。比较报文的出队列速率从其队列到链路的容量来标记加速或刹车。因此，路由器不应该查看当前的排队速率，而应该根据一个RTT中预期的排队速率发出变化信号，以更好地匹配链路容量。我们提出了使ABC与非ABC路由器共存的技术，并与穿越瓶颈ABC路由器的遗留流公平地共享带宽(§4)。当瓶颈路由器为ABC时，ABC流具有高利用率和低排队延迟，而当瓶颈路由器为非ABC路由器时，ABC流切换到Cubic。ABC与ABC流和非ABC流公平竞争。

如何确定用户在给定时间的链接率?我们开发了一种估算Wi-Fi链路速率的方法，并通过实验验证了其准确性。

### 2动机

端到端拥塞控制的局限性：在这种时期，所有端到端方案都必须采取某种形式的“盲目”加息。但是对于具有大动态速率范围的网络来说，正确调整这个速率增长是非常困难的:如果它很慢，吞吐量会受到影响，但是使它太快会导致超调和大的队列延迟对于试图限制队列积累的方案

AQM方案不意味着增加：AQM方案可以在瓶颈链路的缓冲区被填满之前(通过ECN或drop)发出拥塞信号，从而减少延迟，然而，AQM方案并不表示利率上升。端到端和基于aqm的方案都很难准确地跟踪时变的无线链路速率。

显式拥塞控制的部署挑战:像XCP和RCP这样的方案需要对数据包头、路由器和端点进行重大更改。尽管这些更改在技术上是可行的，但在实践中，它们会带来重大的部署挑战。

设计目标：**快速变化无线链路的控制算法**：我们设计了ABC的控制算法，专门用于处理无线链路的快速带宽变化和分组传输行为(例如，MAC层的帧批处理)。

**不修改包头**:ABC重新利用现有的ECN[41]位来向发送方的拥塞窗口发送增加和减少的信号

**与遗留瓶颈路由器共存**:对于瓶颈链路不是无线链路而是路径上其他地方的非ABC链路的场景，ABC是健壮的。当非ABC路由器成为瓶颈时，ABC发送者忽略来自无线链路的窗口增加反馈，并确保他们发送的速度不超过瓶颈链路的公平份额。

**与传统传输协议共存**:ABC路由器确保ABC流和非ABC流公平地共享无线瓶颈链路。

### 3设计

ABC是一种基于窗口的协议:发送方将传输的数据包数量限制在当前拥塞窗口。基于窗口的协议对突发拥塞的反应比基于速率的协议[11]更快。在无线链路上，当容量下降并且发送方停止接收ack时，ABC将立即停止发送数据包，从而避免进一步的队列积累。

ABC发送方根据ABC路由器的明确反馈调整窗口大小。ABC路由器使用其当前估计的链路速率和排队延迟来计算目标速率。然后，路由器在每个数据包中设置一个比特的反馈，以引导发送者达到目标速率。每个比特在ACK中由接收方回显给发送方，它向发送方的拥塞窗口发出信号，要么增加一个包(“加速”)，要么减少一个包(“刹车”)。

ABC的目的是用少量带宽换取大量延迟减少，阈值dt确保目标速率不会对队列延迟的小幅增加做出反应。

### 4公平性

ABC流应该对其路径上存在的非ABC瓶颈具有鲁棒性，并与共享ABC路由器的非ABC流公平地共享资源。

#### 4.1非abc路由器部署

我们增加了ABC发送器以维护两个拥塞窗口，一个用于跟踪ABC路由器上的可用速率(wabc)，另一个用于跟踪非ABC瓶颈上的速率(wnonabc)。wabc按照式(3)服从加速/刹车，而wnonabc则遵循像Cubic[23]这样的规则，并响应drop和ECN信号，ABC发送者必须发送数据包以匹配两个窗口中较低的那个。

#### 4.2用ECN位复用

ABC发送所有设置加速(01)的数据包，ABC路由器通过将位翻转到10来发出刹车信号。01和10都表示支持ecn的传输到支持ecn的传统路由器，后者将继续使用(11)来表示拥塞。

#### 4.3ABC路由器上的非ABC流

ABC路由器将ABC报文和非ABC报文隔离在不同的队列中。ABC路由器分别为ABC队列和非ABC队列分配权重，并根据权重比例调度来自队列的报文。此外，ABC的目标费率计算只考虑ABC在链路容量中所占的份额(由权重控制)。挑战在于设置权重，以确保长期运行的ABC流和非ABC流的平均吞吐量是相同的，而不管有多少流。

先前的显式控制方案使用TCP损失率方程(XCP)或通过僵尸列表(RCP)估计流的数量来解决这个问题。依靠TCP方程需要足够的损失率，并且不能处理BBR这样的流。RCP的方法不处理短流。

### 5估计链路容量

我们描述了ABC路由器如何估计计算目标速率的链路容量(§3.1.2)。我们提出了一种利用Wi-Fi MAC层内部工作原理的Wi-Fi技术，并讨论了蜂窝网络的选项。

#### 5.1wifi

我们现在提出了我们的技术的两种变体:(1)当路由器使用每个用户队列来调度不同用户的数据包时，以及(2)当用户在路由器上共享单个FIFO(先进先出)队列时。

#### 5.2蜂窝网络

3GPP蜂窝标准[1]描述了如何使用蜂窝基站的调度信息来计算每用户链路速率

### 6讨论

我们讨论与ABC部署有关的实际问题。

### 7评估

我们通过考虑以下属性来评估ABC:

1. 性能:我们测量了ABC实现低延迟和高吞吐量的能力，并将ABC与端到端方案、AQM方案和显式控制方案进行了比较(![image-20231121185647469](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231121185647469.png)7.3)。
2. 多重瓶颈:我们在多个ABC瓶颈以及ABC和非ABC瓶颈混合的场景中测试ABC(§7.4)。
3. 公平性:我们在与其他ABC和非ABC流程竞争时评估ABC的公平性(§7.5)。
4. 其他注意事项:我们评估了ABC在应用程序受限流和不同网络延迟下的表现。我们还演示了ABC对实际应用程序性能的影响(§7.6)。

### 8相关工作

ABC通过直接准确估计基站的链路容量来避免这些问题。与ABC不同的是，MTG不能保证一个用户的多个流之间的公平性

### 9结论

本文提出了一种新的简单的时变无线链路显式拥塞控制协议ABC。ABC路由器使用单个比特来标记每个数据包的“加速”或“刹车”，这使得发送者稍微增加或减少他们的拥塞窗口。路由器使用这种简洁的反馈来快速引导发送者达到期望的目标速率。ABC优于现有最好的显式流控制方案XCP，但与XCP不同的是，ABC不需要修改数据包格式或用户设备，使其更容易部署。ABC也是可增量部署的:ABC可以在多个ABC和非ABC瓶颈中正确运行，并且可以与共享相同瓶颈链路的ABC和非ABC流量公平共存。

## 拥塞控制综述

**cubic**：稳健，带宽利用率

对于DCN网络，当使用BBR时，短和长TCP流都具有最高的吞吐量。对于蜂窝网络，采用C2TCP时，长流的吞吐量最好;相比之下，对于短流量，Westwood是最好的。对于广域网上的长流量，使用CUBIC是最好的，但对于短流量，BBR具有最高的吞吐量。有两种情况可以触发Libra退出勘探阶段，即两个候选决策之间的分歧足够大(即 |𝑥𝑟𝑙 −𝑥𝑐𝑙 | ≥ 𝑡ℎ)，或者达到勘探阶段的最大持续时间。这两个案例都表明，现在是重新评估基础cca决策的时候了，以便及时响应不断变化的网络条件。

**BBR Cubic** **Vegas** **Illinois Westwood** C2TCP

### 目前备选cubic，BBR，copa/vegas二选一，westwood，pcc

BBR, Cubic, C2TCP, Vegas, Illinois和Westwood

C2TCP基于CUBIC，所以它的具体参数与CUBIC相同。

C2TCP在WAN和蜂窝网络中实现高性能，但在dcn中表现不佳;BBR的吞吐量在DCNs中非常高，但在蜂窝网络中非常低。

即专门为有线网络设计的面向吞吐量的cca(例如CUBIC)通常在蜂窝网络中表现不佳[1,3,34,40]。

因为BBR相对于小缓冲区场景下基于损失的CCA具有优势[13,21]。

### cubic

CUBIC（Cubic）是一种TCP拥塞控制算法，其特点和适用场景如下：

特点：
1. **拥塞窗口管理：** CUBIC利用拟立方函数调整拥塞窗口大小，以平滑地管理网络拥塞。
2. **快速恢复：** 它允许快速恢复，在发生数据包丢失时能够快速适应网络状况。
3. **公平性：** CUBIC致力于公平共享带宽，确保多个连接在网络中能够公平竞争带宽资源。
4. **稳定性：** 它通过平滑的拥塞窗口调整和快速的恢复机制提高了网络的稳定性，减少了拥塞对网络性能的影响。

适用场景：
1. **高延迟网络：** CUBIC在高延迟网络中表现良好，能够有效管理拥塞，确保流量稳定传输。
2. **高带宽网络：** 由于其公平性特点，CUBIC适用于需要公平共享带宽资源的高带宽网络环境。
3. **稳定性要求高的网络：** 对于对网络稳定性要求较高的应用场景，CUBIC可以提供较为平滑的拥塞控制策略，确保网络流量的稳定性和可靠性。

这些特点和适用场景使CUBIC成为处理高延迟、高带宽网络中拥塞控制的有效工具。

### BBR

BBR（Bottleneck Bandwidth and Round-trip propagation time）是一种TCP拥塞控制算法，其特点和适用场景如下：

特点：
1. **带宽探测准确：** BBR能够准确探测带宽，根据网络状况动态调整发送速率，以实现更高的带宽利用率。
2. **低延迟需求：** 它适用于对低延迟有严格要求的场景，通过优化拥塞控制算法，能够在保证带宽利用率的同时降低网络延迟。
3. **适应性强：** BBR能够根据网络条件的动态变化做出及时调整，适应不同网络环境下的传输需求。
4. **高带宽利用率：** 该算法能够充分利用带宽资源，提高网络传输的吞吐量和效率。

适用场景：
1. **高速网络传输：** BBR适用于高速网络传输场景，能够充分利用网络带宽资源，提高数据传输效率。
2. **视频流媒体传输：** 对于需要低延迟且高带宽利用率的视频流媒体传输，BBR能够保证视频流畅播放和传输的稳定性。
3. **数据中心网络：** 在数据中心的高吞吐量传输场景下，BBR可以有效地管理网络拥塞，提高数据传输的效率和稳定性。

这些特点和适用场景使BBR成为处理高速网络传输和低延迟要求场景中的有效拥塞控制算法。

### Vegas

Vegas是一种拥塞控制算法，其特点和适用场景如下：

特点：
1. **延迟敏感：** Vegas能够根据网络延迟情况调整数据传输速率，保证数据传输的稳定性和低延迟。
2. **公平性：** 该算法能够公平地共享网络带宽资源，避免出现某些连接过度占用带宽而导致其他连接传输缓慢的情况。
3. **快速收敛：** Vegas能够快速适应网络环境的变化，并根据实时的网络拥塞情况快速调整数据传输速率。
4. **灵活性：** 该算法具有一定的灵活性，能够适应不同网络环境下的拥塞情况，并根据实际情况做出相应调整。

适用场景：
1. **实时性要求高的应用：** Vegas适用于对实时性要求较高的应用场景，如视频会议、在线游戏等，能够保证低延迟的数据传输。
2. **数据中心网络：** 在数据中心的高吞吐量传输场景下，Vegas能够保证公平共享带宽资源，避免某些连接占用过多带宽而导致其他连接传输缓慢。
3. **交互式应用：** 对于交互式应用，如实时互动系统和远程控制系统，Vegas能够保证数据传输的稳定性和低延迟，提高用户体验。

这些特点和适用场景使Vegas成为处理对网络延迟和公平共享带宽资源要求较高的场景中的有效拥塞控制算法。

###  **Illinois **

Illinois是一种拥塞控制算法，其特点和适用场景如下：

特点：
1. **低延迟：** Illinois算法通过动态调整拥塞窗口大小来实现低延迟的数据传输，适用于对延迟要求较高的应用场景。
2. **公平性：** 该算法注重公平共享网络带宽资源，避免某些连接占用过多带宽而导致其他连接传输缓慢，确保网络资源的公平分配。
3. **快速收敛：** Illinois能够快速适应网络环境的变化，根据实时的网络拥塞情况快速调整数据传输速率，保证网络的稳定性和可靠性。

适用场景：
1. **交互式应用：** Illinois适用于对网络延迟要求较高的交互式应用场景，如实时互动系统、远程控制系统等，能够保证数据传输的稳定性和低延迟。
2. **实时数据传输：** 在需要实时传输数据的场景下，如音视频传输、远程会议等，Illinois能够保证数据传输的稳定性和低延迟，提高数据传输的效率和质量。
3. **高吞吐量传输：** 在需要高吞吐量传输的场景中，Illinois能够保证公平共享带宽资源，避免某些连接占用过多带宽而导致其他连接传输缓慢，确保网络资源的公平分配。

这些特点和适用场景使Illinois成为处理对低延迟和公平共享带宽资源要求较高的场景中的有效拥塞控制算法。

### **Westwood**

拥塞控制协议Westwood的特点和适用场景如下：

特点：
1. **窗口自适应：** Westwood采用自适应的拥塞窗口调整机制，能够根据网络拥塞程度和带宽变化灵活调整发送窗口大小，提高网络传输效率。
2. **快速恢复：** 当网络出现拥塞时，Westwood能够快速进行拥塞恢复，通过合理的重传策略和窗口调整策略迅速恢复数据传输，降低数据传输的延迟。
3. **带宽利用率高：** 该协议能够有效地利用网络带宽资源，提高数据传输的吞吐量和效率，使网络传输更加高效稳定。

适用场景：
1. **高速长距离传输：** Westwood适用于高速长距离传输的场景，能够通过合理的窗口自适应机制和快速恢复机制提高长距离传输的稳定性和可靠性。
2. **大文件传输：** 在需要传输大文件的场景中，Westwood能够有效利用网络带宽资源，提高数据传输的速度和效率，保证大文件的快速传输。
3. **高负载网络环境：** 该协议适用于高负载网络环境，能够根据网络拥塞情况自适应调整发送窗口大小，保证数据传输的稳定性和可靠性。

这些特点和适用场景使得Westwood成为处理长距离高速网络传输以及对带宽利用率要求较高的场景中的有效拥塞控制算法。

## 调度机制设计

核心：考虑用户体验

核心性能实现：延迟敏感（RTT），吞吐量敏感（bw），丢包敏感（lossrate），转发敏感？（*PPS*，全称是 Packet Per Second（包 / 秒），表示以网络包为单位的传输速率，一般用来评估系统对于网络的转发能力。）

PPS（Packet Per Second）敏感型应用通常涉及需要快速处理数据包的场景。这包括：

1. **网络路由器和交换机**：这些设备需要快速转发大量数据包以确保网络的高效运行。

2. **防火墙和入侵检测系统**：这些系统需要快速检查和过滤网络流量中的数据包，以便保护网络安全。

3. **高频交易系统**：金融交易平台需要以极高的速度处理数据包，以便快速响应市场变化并执行交易。

4. **实时通信应用**：视频会议、在线游戏和语音通话等实时通信应用需要快速传输和处理数据包，以保证良好的用户体验。

这些应用通常需要快速而可靠的网络传输和数据包处理能力，因此对于PPS具有高度敏感性。

为了根据应用程序敏感的需求进行调研，并将敏感参数映射到传输层，你需要首先确定应用程序的敏感需求。以下是一些常见的应用程序敏感需求及其在传输层与拥塞相关的参数的量化示例：

1. **延迟敏感**：在线游戏，cubic
   - 传输层参数：往返时间（RTT）
   - 拥塞相关参数：拥塞窗口大小、丢包率

2. **吞吐量敏感**：云计算，BBR
   - 传输层参数：带宽、吞吐量
   - 拥塞相关参数：拥塞窗口大小、拥塞控制算法

3. **丢包敏感**：视频会议：Vegas
   - 传输层参数：丢包率
   - 拥塞相关参数：拥塞窗口大小、丢包恢复机制

4. **安全性敏感**：网络交易平台
   - 传输层参数：加密算法
   - 拥塞相关参数：拥塞控制算法的安全性特性

5. **带宽敏感**：
   - 传输层参数：带宽限制
   - 拥塞相关参数：流量控制机制、拥塞窗口大小

根据具体的应用需求，你可以进一步量化这些参数。例如，延迟敏感的应用可能需要具有低 RTT 和较小的拥塞窗口，而吞吐量敏感的应用可能需要较大的带宽和相应的拥塞控制算法。

## 延迟和RTT有什么区别

延迟通常指数据在传输过程中所需的时间，包括传播时间、处理时间和排队等待时间。而往返时延（RTT）是指数据包从发送端点到接收端点并返回所需的时间。RTT由往返路径上的延迟和处理时间组成，是衡量网络通信性能的关键指标[[3](https://blog.csdn.net/wangjianno2/article/details/50572351)][[4](https://www.modb.pro/db/616210)][[5](http://129.226.226.195/post/16962.html)].
